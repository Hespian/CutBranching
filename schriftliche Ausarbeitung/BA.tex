\documentclass[]{article}

\usepackage[tmargin=30mm,bmargin=30mm,lmargin=30mm,rmargin=30mm]{geometry}

\usepackage{latexsym,amsmath,amssymb,amsthm,mathtools,textcomp}
\usepackage{hyperref}
%\usepackage{bibgerm}

\usepackage[ruled,vlined,linesnumbered,norelsize]{algorithm2e}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}

\title{}
\author{}

\begin{document}

%\maketitle

%\begin{abstract}

%\end{abstract}

\section{Introduction} \label{sec1}
\subsection{Motivation}
The \emph{maximum independent set} problem is a classic NP-complete graph problem \cite{GareyJohnson} and therefore has been well studied over the last decades. Recent events like the PACPE 2019 implementation challenge \cite{bibitem} show that maximum independent set and related problems are still of interest to researchers today. Given an undirected graph, the problem is to find a set of pairwise non-adjacent vertices of largest cardinality.\paragraph{}
Applications of maximum independent set and its complementary problems \emph{minimum vertex cover} and \emph{maximum clique} cover a variety of fields including computer graphics \cite{CG}, network analysis \cite{NW}, rout planning \cite{RP} and computational biology \cite{BIO1, BIO2}, among others. In computer graphics for instance, small vertex covers can be used to optimize the traversal of mesh edges in a triangle mesh. The dual graph of such a mesh is the graph that contains a vertex for each triangle and an edge between triangles that share a face. A minimum vertex cover in the dual graph corresponds to a minimal number of triangles that contain every mesh edge of the mesh. Unfortunately, due to the complexity of those problems, finding exact solutions to most real-world instances is computationally infeasible. However, a lot of work is invested into finding new techniques to handle the complexity. \paragraph{}
One of the best known techniques for finding exact solutions to those problems, both  in theory and practice, are branch and reduce algorithms. Such algorithms are generally based on kernelization. This means applying a set of reduction rules to decrease the complexity measure (in most cases the size) of an instance while still preserving the solvabilty. A solution to the original instance can then be constructed from a solution of the reduced instance in subexponential time. If an instance can not be reduced further, the algorithm branches into at least two subproblems of lower complexity which are then solved recursively. Branch and reduce algorithms also use problem specific upper and lower bounds to a solution and prune the search space by eliminating solutions which do not satisfy those bounds. \paragraph{}
So far most studies on branch and reduce algorithms for the maximum independent set problem solely focused on finding new and improved reduction rules and lower or upper bounds, respectively. There are very few publications regarding different branching strategies. However, a comparison of three simple branching strategies by Akiba and Iwata \cite{AkibaIwata} shows that the branching strategy can have a huge impact on the running time of an algorithm.

\subsection{Contributions}

This thesis experimentally examines various different branching strategies for maximum independent set. We essentially follow two main approaches. The first approach is to branch on vertices that decompose the graph and then solve the resulting connected components independently. The second approach is to destroy complex structures by branching on vertices such that the structure can be reduced by kernelization afterwards. We implement a variety of different branching strategies following both approaches and compare them to the branching strategy used by the state of the art branch and reduce algorithm for minimum vertex cover proposed by Akiba and Iwata \cite{AkibaIwata}. For testing we use instances from multiple graph classes.\paragraph{}
Branching strategies following the first approach can also be used for other graph problems, but we did not evaluate this. We also did not analyze any of the branching strategies on a theoretical level.  



\subsection{Structure of Thesis}

Following the brief introduction (Section \ref{sec1}), in Section \ref{sec2} we introduce the notation and problem definitions used throughout this Thesis. Here, we also explain the two variants of a branch and reduce algorithm for maximum independent set that we used as a basic framework for testing our branching strategies. In particular we define the reduction rules used by the algorithm.\\
Section \ref{sec3} gives an overview of related work focusing on branching strategies used by other branch and reduce algorithms for maximum independent set or the equivalent problems minimum vertex cover and maximum clique.
In Section \ref{sec4} we outline our approaches and explain the implemented branching strategies in detail. Section \ref{sec5} contains the experimental results. We start by explaining our testing methodology and then state our results. Subsequently, we compare all branching strategies to each other and discuss the effectiveness of our approaches. Finally, in Section \ref{sec6} proposals for future work will be discussed based on the results of the Thesis.

\section{Preliminaries} \label{sec2}

This section introduces basic notation and problem definitions used throughout this Thesis.

\subsection{Basic Definitions}

An undirected graph $G=(V,E)$ is a tuple of a set $V$ of vertices (also called nodes) and a set $E\subseteq \binom{V}{2}$ of edges. Two vertices $v,u\in V$ are called \textit{adjacent} or neighbors if they are connected by an edge, i.e. $\{v,u\}\in E$. The set $N(v) := \{u \in V\;|\; \{v,u\}\in E \}$ of all neighbors of a vertex $v\in V$ is called the \textit{neighborhood} of $v$ and $N[v] := N(v) \cup \{v\}$ denotes to the \textit{closed} neighborhood of $v$. The number $d(v):=|N(v)|$ is called the degree of a vertex. The neighborhood of a set of vertices $S$ is defined as $N(S) := \bigcup_{v\in V}N(v)\setminus S$ and $N[S] := \bigcup_{v\in V}N[v]$ denotes the closed neighborhood of a set. For a vertex $v\in V$ we define $N^2(v) := N(N[v])$. For a subset $S\subseteq V$ the graph $G_S=(S, E\cap\binom{S}{2})$ is called the subgraph \textit{induced by} $S$.\paragraph{}
A subset $I\subseteq V$ is called an \emph{independent set} (IS) of $G$, if no two vertices from $I$ are adjacent, so formally if $\forall v,u\in I: \{v,u\}\notin E$. A \emph{maximum independent set} (MIS) of $G$ is an independent set of largest cardinality. The size of a maximum independent set is called the \emph{independence number} of $G$ and is denoted by $\alpha(G)$. \\
A subset $S\subseteq V$ is called a \emph{vertex cover} of $G$ if for all neighbors $v$ and $u$ in $G$ either $v$ or $u$ (or both) is in $S$, so formally if $\forall \{v,u\}\in E: v\in S \vee u\in S$. A \emph{minimum vertex cover} of $G$ is a vertex cover of minimal cardinality. If $I$ is a (maximum) independent set in $G$, then $V\setminus I$ is a (minimum) vertex cover in $G$.\\
A subset $C\subseteq V$ is called a \emph{clique} of $G$, if any two vertices from $C$ are adjacent, so formally if $\forall v,u\in C:\{v,u\}\in E$. A \emph{maximum clique} of $G$ is a clique of largest cardinality. If $I$ is an (maximum) independent set in $G$, then $I$ is also a (maximum) clique in the complement graph $\overline{G}=(V, \binom{V}{2}\setminus E)$.\paragraph{}
A \textit{path} $P=(v_1,\dots,v_k)$ is a sequence of distinct vertices in $G$ such that $\{v_i,v_{i+1}\}\in E$ for all $i \in \{1,\dots,k-1\}$. A subgraph of $G$ induced by a cardinality maximal subset of the vertices such that any two vertices are connected by a path is called a \textit{connected component}. A graph that contains only one connected component is called \textit{connected}. \paragraph{}
A partition of $V$ into $(S,T)$ is called a \text{cut} and the set $C = \{\{v,u\}\in E\;|\;v\in S,\; u\in T\}$ is called its \textit{cut set}. For two vertices $s$ and $t$, a cut $(S, T)$ such that $s\in S$ and $t\in T$ is called a $s$-$t$-cut. A subset $S\subset V$ is \textit{called} a separator, if $G-S := (V\setminus S, E\cap\binom{V\setminus S}{2})$ has more connected components than $G$ i.e. the removal of $S$ from the graph splits at least one connected component of $G$. Analogous to cuts, a vertex separator that separates two vertices $s$ and $t$ is called a $s$-$t$-separator.

\subsection{Algorithm Framework}

In this Thesis we solely focus on testing new branching strategies. Thus, we do not implement our own branch and reduce algorithm but rather use a state of the art algorithm for minimum vertex cover by Akiba and Iwata \cite{AkibaIwata} as a basis, and modify the branching step within it. Since minimum vertex cover and maximum independent set are complementary problems, the algorithm can be used to find the latter one by just inverting its output. This subsection briefly covers an overview of the algorithm (Algorithm \ref{alg:bb}) from the perspective of maximum independent set.\paragraph{}
Given a graph as input, the algorithm starts with the kernelization step, i.e., reducing the instance's complexity by applying a set of reduction rules (described in \ref{red}). Next, the algorithm tries to prune the current branch by using different upper bounds to an optimal solution. If pruning was not successful but the reduced graph is empty, the algorithm just returns the current best solution size. Otherwise, the algorithm searches for an optimal solution in every connected component of the graph independently. This is denoted as the decomposition step. If a connected component can not be reduced further, the algorithm performs a branching step. In the branching step the algorithm uses the branching strategy to choose a vertex $v$ and then branches into two subinstances. The first case is to include $v$ into the current solution and the second case is to exclude $v$ from the current solution, including its neighbors instead. Both subinstances are then solved recursively and the current optimal solution gets updated accordingly. The algorithm also makes use of branching rules covered in \ref{red} (not to confuse with the actual branching strategy) that sometimes allow further reductions on branching. The \textit{packing} branching rule manages a set of constraints which is updated on every branch and reduction step, and therefore is also handed to the recursive subcalls. For clarity, we omit the details of this in the pseudo code and refer the reader to the original article by Akiba and Iwata \cite{AkibaIwata}. \paragraph{}  
All of our branching strategies branch on a single vertex in each branching step. Thus, in the algorithm we only need to change the method that selects the vertex to branch in most cases. Some of our strategies also maintain additional information which gets updated in each branching step and gets distributed to subinstances accordingly. Other branching strategies require structural information which is obtained during each kernelization step. In those cases we also modify the decompose step or the reduction rules, respectively. \paragraph{}
Since our first approach is to branch on vertices that decompose the graph, we also test a slightly modified version of the algorithm where we add an additional connected components check before the kernelization step. This way, if the graph gets disconnected in a branching step, the instance is decomposed and kernelization afterwards becomes more efficient.

\begin{algorithm}
	\caption{branch \& reduce algorithm for \textsc{Max Independent set} -- Akiba and Iwata \cite{AkibaIwata}}\label{alg:bb}
	\SetKwFunction{S}{Solve}
	\DontPrintSemicolon
	
	\KwIn{A graph $G$, current solution size $c$, current best solution size $k$ }	
	\S{$G,c,k$}
	\Begin{
		$G,c \leftarrow \text{Reduce}(G, c)$ \tcp*{Kernelization}
		$l \leftarrow \text{UpperBound(G)}$ \tcp*{Calculate upper bounds}
		\lIf{$c+l\leq k$}{\Return $k$} \tcp*{Prune current branch}
		\lIf{$G \text{ is empty}$}{\Return $k$}\;
		\uIf{$G$ is not connected}{
			\ForEach{connected component $G_i$ \KwSty{of} $G$}{
				$c \leftarrow c\text{ }+$ \S{$G_i,0,k-c$}  \tcp*{Solve connected components independently}
			}
			\Return $\max\{c, k\}$
		}
		$(G_1,c_1,k),(G_2,c_2,k) \leftarrow \text{Branch}(G,c)$ \tcp*{Branch on a vertex $v$ into two subcases}
		$k\leftarrow \max$$\{$$k, \, $ \S{$G_1,c_1,k$}$\}$\;
		$k\leftarrow \max$$\{$$k, \, $ \S{$G_2,c_2,k$}$\}$
		
		\Return $k$
		
	}
	
	\KwOut{the size $k$ of a maximum independent set or the size $n-k$ of a minimum vertex cover}
	
\end{algorithm}
\subsection{Reduction and Branching Rules} \label{red}

In this subsection we explain the various reduction and branching rules used in the kernelization and branching steps of the algorithm by Akiba and Iwata \cite{AkibaIwata}. We formulate all reduction and branching rules for the maximum independent set problem, although the algorithm was originally designed for minimum vertex cover and therefore uses the equivalent counterparts of those rules. The algorithm also keeps track of the order in which the reduction rules are applied, such that a correct solution to the original instance (a specific maximum independent set and not only the size of one) can be constructed from a solution in the reduced graph later on. For clarity, we omit the details of this.
\paragraph{}
The first reduction rule, the degree one reduction, is actually completely contained in the dominance and unconfined reductions (covered later in this section). 
Nevertheless, due to its low computational costs, it is used in addition to those more general rules. 

\begin{theorem} (Degree One Reduction) Let $G=(V,E)$ be a graph with a vertex $v$ of degree one and let $u$ be the only neighbor of $v$. Then, there is a maximum independent set that includes $v$ and therefore excludes $u$.	
\end{theorem}
\begin{proof}
	Consider a maximum independent set $I$ in $G$. $I$ has to contain either $u$ or $v$ because otherwise $I\cup\{v\}$ would be an \textit{independent set} of larger size. If $I$ contains $u$, it can not contain $v$ and thus, $(I\setminus\{u\})\cup\{v\}$ is another  maximum independent set that include $v$ and excludes $u$.
\end{proof}

At the beginning of each kernelization step, the algorithm searches for vertices of degree one, includes them into the current solution and deletes their neighbors from the graph. The algorithm also checks whether removing a neighbor from the graph produces new degree one vertices, and in this case further applies the degree one reduction. \paragraph{}
The next reduction rule deals with vertices of degree two that are not part of a triangle, i.e., whose neighbors are not adjacent and was introduced by Chen et al. \cite{ChenDeg2}.

\begin{theorem} (Degree Two Folding) Let $G=(V,E)$ be a graph with a vertex $v$ of degree two and let $u,w$ be the neighbors of $v$. Let $G'=(V',E')$ be the graph with $V'=(V\setminus N[v])\cup \{x\}$ where $x\notin V$ and $E'=(E\cap \binom{V'}{2})\cup\{ \{x,y\}\;|\;y\in(N(u)\cup N(w))\setminus\{v\} \}$ and let $I'$ be a maximum independent set of $G'$. Then,
	
	\[I=\begin{dcases}
	I'\cup \{v\} & \text{, if }x\notin I'\\
	(I'\setminus\{x\})\cup \{u,w\}) & \text{, else}%x\in I'
	\end{dcases}\]\\
	is a maximum independent set in $G$.
\end{theorem}
\begin{proof}
	Consider any maximum independent set $I$ of $G$. If $I$ contains $v$, then it can not contain $u$ and $w$. Thus, $I\setminus\{v\}$ is an \textit{independent set} in $G'$ of size $|I| - 1$. Otherwise, if $I$ does not contain $v$, $I$ has to include at least one neighbor of $v$ (since $I$ is maximal). If $I$ contains only one neighbor of $v$, removing this neighbor from $I$ yields an \textit{independent set} in $G'$ of size $|I|-1$. If $I$ contains both $u$ and $w$, then $I'=(I\setminus\{u,w\})\cup\{x\}$ is an independent set in $G'$ of size $|I|-1$. So, in total $\alpha(G')\geq \alpha(G) -1$. On the other hand, $I$ constructed from a maximum independent set $I'$ of $G'$ is an \textit{independent set} of $G$ of size $|I| = |I'|+1$ and thus, $I$ is a maximum independent set in $G$.
\end{proof}

So, if the algorithm finds a vertex $v$ of degree two whose neighbors are not adjacent, the algorithm reduces the size of the graph by removing $N[v]$ adding a new vertex connected to $N^2(v)$ instead. This procedure is called \textit{folding} the closed neighborhood of $v$, hence the name degree two folding.\paragraph{}
The next two reduction rules can be used to delete single vertices that are not required in a maximum independent set. The first of those rules (the dominance Reduction) is fully contained in the second rule (the unconfined reduction) and therefore the algorithm only uses the latter one. However, we used the concept of dominance to design one of our branching strategies. For this reason the dominance reduction rule is also featured in detail.

\begin{definition} (Dominance)
	In a Graph $G=(V,E)$ a vertex $u$ is called dominated by a neighbor $v$, if $N[u]\subseteq N[v]$.
\end{definition}


\begin{theorem} (Dominance Reduction) In a Graph $G=(V,E)$, if a vertex $u$ is dominated by a neighbor $v$, then, there always exists a maximum independent set that does not include $v$, i.e.\[\alpha(G)=\alpha(G-v)\]
\end{theorem}
\begin{proof}
Consider a maximum independent set $I$ that does contain $v$. Since $N[u]\subseteq N[v]$, $I$ can neither contain $u$ nor any of its neighbors. But then, clearly, $I' = (I\setminus\{v\})\cup\{u\}$ is an \textit{independent set} of the same size as $I$ that does not include $v$.
\end{proof}
Thus, if a vertex $v$ dominates another vertex $u$, one could safely remove $v$ from the graph without compromising the solvabilty of the instance.\paragraph{}
The core idea of the \textit{unconfined reduction} proposed by Xiao and Nagamochi \cite{XiaoNagamochi} is to detect a vertex that is not required for a maximum independent set and therefore can be removed from the graph by algorithmically contradicting the assumption that every maximum independent set contains the vertex.
\begin{definition} (Removable Vertex)
	In a graph $G=(V,E)$ a vertex $v$ is called removable, if 
	\[\alpha(G) = \alpha(G-v)\]
\end{definition}

\begin{definition} (Child, Parent) In a Graph $G=(V,E)$ with an \textit{independent set} $I$, a vertex $v$ is called a child of $I$, if $|N(v)\cap I| = 1$ and the unique neighbor of $v$ in $I$ is called the parent of $v$.
\end{definition}

\begin{theorem} \label{unconfined}
	In a graph $G=(V,E)$ let $S$ be an independent set that is not maximal but is contained in every maximum independent set of $G$ and let $v$ be any child of $S$. Then, every maximum independent set includes at least one vertex from $N(v)\setminus N[S]$.
\end{theorem}
\begin{proof}
	Assume that there is a maximum independent set $I$ that includes $S$ but no vertex from $N(v)\setminus N[S]$ and let $u$ be the parent of $v$ in $S$. Then, $I'=(I\setminus\{u\})\cup\{v\}$ is an independent set of the same size as $I$, since $I$ contains no neighbor of $v$ other than $u$. This contradicts the fact that every maximum independent set includes $S$.
\end{proof}

Based on Theorem \ref{unconfined} Algorithm \ref{alg:unconf} detects so called \textit{unconfined} vertices.

\begin{algorithm}
	\caption{Unconfined -- Xiao and Nagamochi \cite{XiaoUnconfined}}\label{alg:unconf}
	\SetKwFunction{U}{Unconfined}
	\DontPrintSemicolon
	
	\KwIn{A graph $G$, a vertex $v$}
	\U{G, v}	
	\Begin{
		$S \leftarrow \{v\}$\;
		\While{$S\text{ has child }u\text{ with } |N(u)\setminus N[S]|\leq 1$}{
			\eIf{$|N(u)\setminus N[S]| == 0$}{\Return true\tcp*{Contradiction to Theorem \ref{unconfined}}}{
				$\{w\}\leftarrow N(u)\setminus N[v]$\tcp*{By assumption $w$ also has to}
				$S\leftarrow S\cup\{w\}$\tcp*{be contained in every MIS}
			}
		}
			
		\Return $\text{false}$
		
	}
	
	\KwOut{true if $v$ is unconfined, false otherwise}
	
\end{algorithm}


\begin{theorem}(Unconfined Reduction) In a Graph $G=(V,E)$, if Algorithm \ref{alg:unconf} returns true for an unconfined vertex $v$, then, there is always a maximum independent set that does not contain $v$.
\end{theorem}
\begin{proof}
	Assume, that $v$ is included in every maximum independent set. Every vertex added to $S$ by the algorithm is the unique neighbor of a child of $S$. Therefore, by Theorem \ref{unconfined} this vertex also has to be contained in every maximum independent set, and thus can be added to $S$. If the algorithm returns true, then there is a child of $S$ that has no neighbor that can be included in $S$. Thus, by Theorem \ref{unconfined} the assumption that $v$ is included in every maximum independent set was false and therefore $v$ is removable.
\end{proof}
During kernelization, the branch and reduce algorithm uses Algorithm \ref{alg:unconf} to detect and remove unconfined vertices.\paragraph{}
The twin reduction by Xiao and Nagamochi \cite{XiaoUnconfined} deals with pairs of degree three vertices that share the same neighborhood.

\begin{definition}(Twins)
	In a Graph $G=(V,E)$ two vertices $u$ and $v$ are called twins, if $N(u) = N(v)$ and $d(u) = d(v) = 3$.
\end{definition}

\begin{theorem} (Twin Reduction) In a Graph $G=(V,E)$ let vertices $u$ and $v$ be twins. If there is an edge among $N(u)$, then there is always a maximum independent set that includes $\{u,v\}$ and therefore excludes $N(u)$. Otherwise, let $G'=(V',E')$ be the graph with $V'=(V\setminus N[\{u,v\}])\cup\{w\}$ where $w\notin V$ and $E'=(E\cap\binom{V'}{2})\cup \{\{w,x\}\;|\;x\in N^2(u)\})\}$ and let $I'$ be a \textit{maximum vertex cover} in $G'$. Then, 
	
	\[I=\begin{dcases}
	I'\cup \{u,v\} & \text{, if }w\notin I'\\
	(I'\setminus \{w\})\cup N(u) & \text{, else} %w\in I'
	\end{dcases}\]
	is a maximum independent set in $G$.
\end{theorem}
\begin{proof}
	For the first case (there is an edge among $N(u)$) consider a maximum independent set $I$ that does not contain $u$ or $v$. Then, $I$ has to include at least one neighbor of $u$ and $v$, because otherwise $I\cup \{u,v\}$ would be an \textit{independent set} larger than $I$. On the other hand, since there are neighbors of $u$ and $v$ that are adjacent, $I$ can only contain at most two neighbors of $u$ and $v$. But then, $I' = (I\setminus N(u))\cup\{u,v\}$ is an \textit{independent set} of the same size as I that includes $u$ and $v$.\\
	For the second case (no edges among $N(u)$) note, that the reduction produces a set that in both cases contains exactly two vertices more than a maximum independent set in $G'$. Now consider a maximum independent set $I$ in $G$. If $N(u)$ is completely contained in $I$ ($N(U)\subseteq I$), then $I$ can not contain any vertex of $N^2(u)$, i.e., any neighbor of $w$ in $G'$. Thus, $I' = (I\setminus N(u))\cup\{w\}$ is an \textit{independent set} of $G'$ of size $|I| - 2$. Otherwise, $I$ contains at most two vertices from $N(u)\cup\{u,v\}$ (either $u$ and $v$ or two vertices from $N(u)$). But then, $I' = I\setminus(N(u)\cup\{u,v\})$ is also an \textit{independent set} of $G'$ of size $|I|-2$.\\
	In total $\alpha(G) \leq \alpha(G')+2$ and thus, $I$ is a maximum independent set of $G$
	\end{proof}

During the kernelization step, the algorithm searches for twins $u$ and $v$. If there is an edge among $N(u)$, the algorithm includes $u$ and $v$ to the current solution and deletes $\{u,v\}\cup N(u)$. Otherwise, the algorithm still deletes $\{u,v\}\cup N(u)$ introducing a new vertex connected to $N(u)\setminus\{u,v\}$ instead.\paragraph{}
The next reduction rule as well as its special cases were also proposed by Xiao and Nagamochi \cite{XiaoUnconfined}.

\begin{definition} (Alternative Sets)
	In a Graph $G=(V,E)$ two non empty, disjoint subsets $A,B\subseteq V$ are called alternatives, if $|A| = |B|$ and there is a maximum independent set $I$ in $G$ such that $I\cap(A\cup B)$ is either $A$ or $B$.
\end{definition}
\begin{theorem} (Alternative Reduction)
	In a Graph $G=(V,E)$ let $A$ and $B$ be alternative sets. Let $G'=(V', E')$ the graph with $V' = V\setminus(A\cup B\cup (N(A)\cap N(B)))$ and $E' = (E\setminus\binom{A\cup B\cup (N(A)\cap N(B))}{2} \cup \{ \{x,y\}\;|\; x\in N(A)\setminus N[B], y\in N[B]\setminus N(A) \}$ and let $I'$ be a maximum independent set in $G'$. Then,
	
	\[I=\begin{dcases}
	I'\cup A & \text{, if } (N(A)\setminus N[B]) \cap I' = \emptyset% N(B)\setminus N[A]\subseteq I'\\
	\\
	I'\cup B & \text{, else if } (N(B)\setminus N[A])\cap I' = \emptyset
	\end{dcases}\]
	is a maximum independent set in $G$.
\end{theorem}
\begin{proof}
	Consider a maximum independent set $I$ in $G$ and without loss of generality let $A\subseteq I$ (by definition $A$ or $B \subseteq I$). Thus, $I\cap ((A\cup B\cup (N(A)\cap N(B))) = A$ and $I\cap(N(A)\setminus N[B])=\emptyset$. Now let $I' = I\setminus A$. $I'$ is an \textit{independent set} in $G'$, since each added edge (from $E'\setminus E$) is incident to a vertex from $N(A)\setminus N[B]$ and $|I'| = |I|-|A|$. This implies $\alpha(G')+|A| \geq \alpha(G)$.\\
	Conversely, let $I'$ be a maximum independent set of $G'$. Obviously, $I'$ is also an \textit{independent set} of $G$. Since vertices from $N(A)\setminus N[B]$ are pairwise adjacent to vertices form $N(B)\setminus N[A]$, $I'$ can only contain vertices from either $N(A)\setminus N[B]$ or $N(B)\setminus N[A]$. But then, $I=I'\cup A$ or $I=I'\cup B$ respectively is an \textit{independent set} in $G$. Thus, $\alpha(G')+|A| \leq \alpha(G)$\\
	In total $\alpha(G')+|A| = \alpha(G)$ and $I$ is a maximum independent set in $G$.
\end{proof}
Note that the \textit{alternative reduction} adds new edges between existing vertices of the graph. For this reason, applying the \textit{alternative reduction} is not beneficial in every case. To counteract this, the algorithm only uses the following special cases of the \textit{alternative reduction}.

\begin{definition} (Funnel)
	In a Graph $G=(V,E)$ two adjacent vertices $u$ and $v$ are called funnels, if $G_{N(v)\setminus\{u\}}$ is a complete graph, i.e, if $N(v)\setminus\{u\}$ is a clique.
\end{definition}
\begin{theorem} (Funnel Reduction) In a Graph $G=(V,E)$ let $u$ and $v$ be funnels. Then, $\{u\}$ and $\{v\}$ are alternative sets.	
\end{theorem}
\begin{proof}
	We have to show that there is a maximum independent set that contains either $v$ or $u$. So, consider a maximum independent set $I$ that excludes both $u$ and $v$. Then, $I$ has to include at least one vertex from $N(v)\setminus\{u\}$, because otherwise $I\cup\{v\}$ would be an \textit{independent set} of larger size. On the other hand, $I$ can only contain at most one vertex $x$ from $N(v)\setminus\{u\}$, since $N(v)\setminus\{u\}$ is a clique. But then, $(I\setminus\{x\})\cup\{v\}$ is an \textit{independent set} of the same size as $I$ that does contain $v$. Thus $\{u\}$ and $\{v\}$ are alternative sets.
\end{proof}

\begin{definition} (Desk)
	In a Graph $G=(V,E)$ a cycle $u_1u_2u_3u_4$ of length four with no chords (i.e., an induced 4-cycle) is called a desk, if each of the vertices has at least degree three, $N(\{u_1, u_3\})\cap N(\{u_2, u_4\}) = \emptyset$ and $|N(\{u_1, u_3\})\setminus \{u_2, u_4\}|\leq 2$ as well as $|N(\{u_2, u_4\})\setminus \{u_1, u_3\}|\leq 2$.
\end{definition}
\begin{theorem}(Desk Reduction) 
	In a Graph $G=(V,E)$ let $u_1u_2u_3u_4$ be a desk. Then, $\{u_1, u_3\}$ and $\{u_2, u_4\}$ are alternative sets.	
\end{theorem}
\begin{proof}
	Consider a maximum independent set $I$ of $G$. If $|I\cap \{u_1,u_2,u_3,u_4\}| > 1$, then clearly\\ $I\cap \{u_1,u_2,u_3,u_4\} $ is either $\{u_1, u_3\}$ or $\{u_2, u_4\}$. Otherwise, without loss of generality $u_2,u_3,u_4\notin I$ and $|I\cap N[\{u_1,u_3\}]|=2$. The last equation holds because $|N(\{u_1, u_3\})\setminus\{u_2,u_4\}| \leq 2$ by definition, and $u_1$ has at least one neighbor in $N(\{u_1, u_3\})\setminus\{u_2,u_4\}$ ($d(u_1)\geq3$). But then, $(I\setminus\{N(\{u_1,u_3\})\})\cup\{u_1,u_3\}$ is an \text{independent set} of the same size as $I$ that does contain $\{u_1, u_3\}$. Thus, $\{u_1,u_3\}$ and $\{u_2, u_4\}$ are alternative sets.
\end{proof}

During kernelization, the algorithm searches for funnels or desks and reduces those structures according to the alternative reduction.\paragraph{}
The algorithm also uses a reduction based on a solution to the LP-Relaxation of maximum independent set.
\begin{gather*}
	\text{maximize} \sum_{v\in V}x_v\\
	0\leq x_v\leq 1 \;\;\; \forall v\in V\\
	x_v + x_u\leq 1 \;\;\; \forall \{u,v\}\in E
\end{gather*}

Nemhauser and Trotter show that there always exists an optimal half integral solution to the LP-Relaxation, i.e., an optimal solution where $x_v\in\{0,\frac{1}{2},1\}$ for all $v\in V$ \cite{NemhauserTrotter}. They also show that given an optimal half integral solution to the LP-Relaxation, there is always a maximum independent set that includes all vertices $v$ with $x_v=1$ and excludes all vertices $u$ with $x_u = 0$. Furthermore, they show that finding an optimal half integral solution can be reduced to computing a \textit{maximum matching} in a bipartite graph. \\
Iwata et al. develop an algorithm that given any optimal half integral solution constructs another half integral solution that minimizes the number of variable with half integral value \cite{IwataOkaYoshida}.\\
The algorithm uses this solution to the LP-Relaxation to reduce the graph and also as an upper bound to an optimal solution.\paragraph{}
Apart from reduction rules, the algorithm also uses branching rules that allow further reductions on branching when certain conditions hold. The first branching rule, mirror branching, was introduced by Fomin et al. \cite{Fomin}. According to Kneis et al. it is potentially useful, if the branching vertex has a rather low degree and thus, most likely has some mirror \cite{Kneis}.

\begin{definition} (Mirror)
	In a graph $G=(V,E)$ a vertex $u$ is called a mirror of a vertex $v$, if $u\in N^2(v)$ and $G_{N(v)\setminus N(u)}$ is a (possibly empty) complete graph, i.e. $N(v)\setminus N(u)$ is a (possibly empty) clique. The set of all mirrors of $v$ is denoted by $\mathcal{M}(v)$ and $\mathcal{M}[v] := \mathcal{M}(v)\cup\{v\}$.
\end{definition} 
\begin{theorem} (Mirror Branching)
	In a graph $G=(V,E)$, if there is no maximum independent set that contains a vertex $v$, then, every maximum independent set also excludes $\mathcal{M}[v]$.
\end{theorem}
\begin{proof}
	Consider any maximum independent set $I$. Then, $I$ has to contain at least two neighbors of $v$ because otherwise, we could get a maximum independent set $I'=(I\setminus N(v))\cup\{v\}$ that includes $v$. Now let $u\in\mathcal{M}(v)$ be a mirror of $v$. Since $N(v)\setminus N(u)$ is a clique, $I$ can only contain at most one vertex from $N(v)\setminus N(u)$. Thus, $I$ contains at least another vertex from $N(v)\cap N(u)$ and therefore has to exclude $u$.
\end{proof}

So, when branching on a vertex $v$, the algorithm finds its mirrors $\mathcal{M}(v)$ and considers two possible cases. The first cases is that there is a maximum independent set that includes $v$ and therefore exclude $N(v)$. And the second case is that no maximum independent set includes $v$. In this case, the vertices from $\mathcal{M}(v)$ can also be discarded from the graph.\paragraph{}
The packing branching rule by Akiba and Iwata \cite{AkibaIwata} is a generalization of the idea behind the satellite branching rule by Kneis et al. \cite{Kneis}. The core idea behind those rules is that when branching in the case of excluding a vertex $v$ from the solution, one can assume that no maximum independent set contains $v$. Otherwise, if there is a maximum independent set that contains $v$, the algorithm finds it in the branch that includes $v$.\\
Based on the assumption that no maximum independent set includes a vertex $v$, constraints for the remaining vertices can be derived. For example, a maximum independent set that does not contain $v$ has to include at least two neighbors of $v$. The corresponding constraint is $\sum_{u\in N(v)}x_u \geq2$, where $x_u$ is a binary variable that indicates whether a vertex is included in the current solution. The algorithm creates such constraints when branching, and updates them accordingly during the kernelization and branching steps. The constraints can then be used to reduce the graph or to prune the current branch when a constraint can not be fulfilled by the current solution.


\newpage
\section{Related Work} \label{sec3}

This section discusses related work. It focuses on presenting branching strategies used by other branch and reduce algorithms for maximum independent set or its equivalent problems minimum vertex cover and maximum clique.\paragraph{}

There are various approaches to tackle the maximum independent set problem and its equivalent problems. These approaches comprise exact as well as heuristic methods. A technique that is frequently used for both exact and inexact algorithms is kernelization. We already covered the most important reduction rules used for kernelization in Subsection \ref{red}.

\paragraph{}
 Due to the NP-Hardness of the maximum independent set problem, inexact algorithms have been well studied in practice. One of the best techniques used for finding large independent sets is local search~\cite{bibid}. Local search algorithms start with an initial solution and then utilize simple operations to iteratively improve the current solution. In practice, local search algorithms often find near optimal solutions very fast. However, most of them can not give any guarantee for the actual quality of a solution. One of the best local search algorithms for maximum independent set, called ARW, was proposed by Andrade et al.~\cite{bibid}. Their algorithm uses the concept of $(j,k)$-swaps, i.e. removing $j$ vertices from the current solution and replacing them with $k$ vertices instead. In every iteration of their algorithm, they perform a $(1,2)$-swap to improve the current solution. To escape local optima, ARW occasionally disturbs the current solution by randomly inserting vertices into it and removing their neighbors instead.

\paragraph{}
Chang et al.~\cite{bibid} developed a linear time kernelization algorithm which reduces vertices of degree one and two. Another variant of their algorithms additionally applies the dominance reduction rule but has only near-linear running time. They also showed that applying their kernelization algorithm iteratively followed by removing vertices of high degree, is going to result in a large initial solution which, in turn, speeds up the ARW local search algorithm.
\paragraph{}
The theoretically most powerful exact exponential time algorithms for maximum independent set are branch and reduce algorithms. These algorithms use kernelization to compute a kernel of the input instance. After that, they branch into subinstances with lower complexity, which are then solved recursively. We now discuss the branching strategies used in various different branch and reduce algorithms.

\paragraph{}
The most common branching strategy used for maximum independent set and minimum vertex cover is branching on a vertex of maximum degree. Fomin et al. gave a theoretical analysis of this using the measure and conquer technique with a weighted degree sum as measure \cite{Fomin}. They showed that choosing a vertex of maximum degree that also minimizes the number of edges in its neighborhood is optimal with respect to their complexity measure. This greedy strategy is also used by the algorithm of Akiba and Iwata\cite{AkibaIwata} and serves as a baseline for comparison in our experiments. Akiba and Iwata already compared this strategy with branching on a vertex of minimum degree and the strategy of choosing a branching vertex at random. Their experiments showed that those strategies are significantly worse than branching on maximum degree vertices.\paragraph{}
Xiao and Nagamochi proposed a branch and reduce algorithm for maximum independent set that, in most cases, branches on a vertex of maximum degree but also uses a special edge branching strategy to handle dense subgraphs \cite{XiaoNagamochi}. Edge branching is based on the principle of alternative subsets (like in alternative reduction). Given an edge $\{u,v\}\in E$ a maximum independent set can only contain $u$ or $v$ but not both of them. So, if there is a maximum independet set that includes $u$ or $v$, then $\{u\}$ and $\{v\}$ are alternative sets. Thus, branching on the edge $\{u,v\}\in E$ yields two cases. The first case is to remove both $u$ and $v$ and to search for a maximum independent set that does not include $u$ and $v$. The second case is to compute the alternative reduction of $\{u\}$ and $\{v\}$, i.e., to remove $\{u,v\}\cup(N(u)\cap N(v))$ and insert an edge $\{x,y\}$ between any nonadjacent vertices $x\in N(u)\setminus N(v)$ and $y\in N(v)\setminus N[u]$ and to search for a maximum independent set that includes either $u$ or $v$.\\
The algorithm by Xiao and Nagamochi uses edge branching in degree bounded graphs on edges $\{u,v\}\in E$, where $|N(u)\cap N(v)|$ is sufficiently large (the concrete values depend on the maximum degree of the graph). \paragraph{}
Bourgeois et al. presented branch and reduce algorithm for maximum independent set that relies on fast algorithms for graphs with low average degree \cite{Bourgeois}. If the average degree of the graph is greater than 4, the algorithm branches on a vertex of maximum degree. Otherwise, if the average degree of the graph is at most 4, they use a specialized algorithm to solve the instance. If there is no vertex with degree of at least 5, this algorithm branches on vertices contained in 3- or 4-cycles.\paragraph{}
Chen, Kanj and Xia developed a branch and reduce algorithm for the problem minimum vertex cover parameterized by the size $k$ of the vertex cover, i.e., the problem of finding a vertex cover of size not larger than $k$ \cite{ChenXiaKanj}. In their algorithm, they use the concept of so called tuples and good pairs. A good pair is a pair of adjacent vertices that are advantageous for branching (the details are omitted here). A tuple is a set $S$ of vertices together with the number of vertices in $S$ that can be excluded from a minimum vertex cover. This information can be exploited during the branching to eliminate additional vertices. For example, consider the pair $(\{u,v\}, 1)$. We know that either $u$ or $v$ can be excluded from a minimum vertex cover and thus, if we include $u$ to the vertex cover, we can exclude $v$. Otherwise, if we exclude $u$ from the vertex cover, we can include $v$. Akiba and Iwata used the same idea in their packing reduction \cite{AkibaIwata}. The algorithm by Chen, Kanj and Xia maintains a set of those structures as well as vertices of high degree and updates them accordingly during kernelization and branching. At each branching step the algorithm chooses the best structure and branches on it.\paragraph{}
Most branch and reduce algorithms for maximum clique use some sort of greedy coloring to find an upper bound to the size of a maximum clique and also to reduce the number of possible vertices for branching. Given a coloring $c:V \rightarrow \mathbb{N}$ and the size $c_\text{max}$ of a current best solution, it is easy to see that for $A = \{v\in V \;|\; c(v)\leq c_\text{max}\}$, $G_A$ can not contain a clique larger than the current best solution. Thus, only vertices from $V\setminus A$ are considered for branching.
\paragraph{}
More sophisticated algorithms use a MaxSAT encoding of maximum clique to achieve better upper bounds and to further reduce the set of branching vertices \cite{LiFangXu,LiJiang}. Maximum clique can be reduced to MaxSAT by introducing a binary variable $x_v$ for every vertex $v\in V$ and a hard clause $\overline{x}_u\lor\overline{x}_v$ for each pair of non adjacent vertices. The set $\{x_v\;|\;v\in V\}$ of unit literals forms the soft clauses. A solution to this MaxSAT instance yields a maximum clique where the value of a variable indicates whether the corresponding vertex is included in the clique or not. A more efficient MaxSAT enconding of maximum clique was introduced by Li and Quan \cite{LiQuan}. Given a partition of $V$ into independent sets (i.e. a coloring), instead of introducing a soft clause for each vertex, they merely formulate a single clause $\bigvee_{x_i\in I_j} x_i$ for each independent set $I_j$ in the partition. Using this encoding they apply techniques from SAT solving like unit propagation and failed literal detection to identify conflicting independent sets. A set of $t$ independent sets is called conflicting if there is no clique of size $t$ in the subgraph induced by those independent sets. If conflicting independent sets are detected, the soft clauses get weakened by conjugating the clauses in a respective manner. Li and Quan show that if a Graph can be partitioned into $k$ independent sets with $t$ disjoint conflicting subsets, then the size of a maximum clique is bounded~by~$k-t$. This way, they achieve an upper bound which is often better than the bound obtained by the coloring.
\paragraph{}
Li et al. use a similar MaxSAT reasoning to reduce the number of vertices that are considered for branching \cite{LiMaxSat}. They initially use a coloring to obtain a partition of $V$ into parts $A$ and $V\setminus A$ where $A$ is defined as $A = \{v\in V \;|\; c(v)\leq c_\text{max}\}$ and $c_\text{max}$ is the size of the current best solution. After that they construct a MaxSAT instance where they add a soft clause for each color class (i.e. the set of vertices with the same color) in $A$. Then, they iteratively add a soft clause $x_{v_i}$ for each vertex $v_i \in V\setminus A$ and apply unit propagation to it. If a conflict is detected, the affected soft clauses get weakened. This process is repeated until no more conflicts are detected or $V\setminus A$ becomes empty. Li et al. show that if there are conflicts for literals $x_{v_1},\dots x_{v_k}$ with $v_i\in V\setminus A$, then the graph induced by $A\cup\{v_1\dots v_k\}$ does not contain a clique larger than the current best solution $c_\text{max}$. Thus, those vertices do not have to be considered for branching.



\paragraph{}
Another approach to decrease the number of branches in branch and reduce algorithms for maximum clique is to choose branching vertices in a specific beneficial order. A common strategy for choosing the branching vertex is to calculate a so called \textit{degeneracy ordering} $v_1 < v_2 < \dots < v_n$ where $v_i$ is a vertex of smallest degree in $G - \{v_1, \dots, v_{i-1} \}$, and to choose the vertices for branching in descending order \cite{CarraghanPardalos}. Li et al. introduced another vertex ordering for branching using maximum independent sets \cite{LiFangXu}. While $G$ is not empty, they repeatedly search for maximum independent sets and remove them from the graph. Then, the vertex ordering is defined in the following way using the degeneracy ordering for tie breaking: For two vertices $u$ and $v$, $u < v$ if $u$ has been removed later than $v$ or if $u$ and $v$ have been removed at the same time but $u < v$ in the degeneracy ordering. 
 
\paragraph{}
Most of the exact algorithm for maximum independent set and its equivalent problems have been analyzed on a theoretical level but are not very well tested on real instances. In practice, the best algorithms use a combination of multiple techniques. For example, the winning solver of the PACE challenge by Hespe, Lamm, Schulz and Strash~\cite{bibid} uses kernelization, iterated local search, a branch and reduce algorithm for vertex cover and a branch and bound algorithm for maximum clique. Their algorithm starts by applying a huge set of reduction rules to obtain a kernel as small as possible. After that, they use iterated local search to find a large initial solution and prime the branch and reduce algorithm with it. Subsequently, the branch and reduce algorithm is run for a short period of time. If no solution is found during that time, they run a branch and bound maximum clique solver on the complement of kernel and, thereafter, on the complement of the original instance. Again, if no solution has been found so far, they re-run the branch and reduce algorithm for a longer time followed by the clique solver if needed.

\newpage
\section{Branching Strategies} \label{sec4}
In this section we outline our approaches for designing the different branching strategies. We also give full details on the implementation itself and on the observations we made during the implementation. As mentioned in the introduction, we follow two main approaches in designing our branching strategies. 
\paragraph{}
The first approach is to decompose the graph by branching. This way, the resulting connected components can be solved independently speeding up kernelization and potentially reducing the total size of the search space. For this thesis we implemented 3 branching strategies using this approach which are described in Section \ref{decomp}.
\paragraph{}
The second approach is to destroy complex structures that can not be reduced by kernelization. Our main idea behind this approach is to identify vertices that prevent a certain reduction rule from being applicable and then branch on them. This way, the respective reduction rule can be applied afterwards and the graph gets reduced further. Also, such vertices can be found during kernelization which is continuously performed before every branching step. Therefore, the overhead of those branching strategies compared to the strategies following our first approach is rather small. We implemented branching strategies that target $x$ different reduction rules and also tested combinations of those. Branching strategies following this approach are covered in section \ref{red_strats}.
\paragraph{}

For almost all of our branching strategies it is not guaranteed that they find a suitable vertex for branching in every branching step. Consequently, all of them use a default branching strategy as a fallback. In our implementation we used branching on a vertex with maximum degree that also minimizes the number of edges among its neighborhood  as default branching strategy. This is also the strategy proposed by Fomin et al. \cite{Fomin} and already implemented in the algorithm by Akiba and Iwata \cite{AkibaIwata}.

\subsection{Branching Strategies Based on Decomposition} \label{decomp}
Since the branch and reduce algorithm that we used as a our basis was designed for branching on a single vertex, our first idea is to find and branch on articulation points  (i.e. cut vertices) of the graph. Articulation points of a graph $G$ are single vertices that form a separator of size one in $G$. Given any spanning tree of $G$, each articulation point $v$ separates the vertices in the subtree rooted by a child of $v$ from the rest of the graph. Thus, in a spanning tree obtained by a depth first search (DFS) there are no back edges from the graph induced by the subtree rooted at this child to the predecessors of $v$ in the DFS tree.

Using this observation, articulation points in a connected graph can be found in linear time using the following algorithm (Algorithm \ref{alg:artic}) based on a depth first search scheme: The algorithm performs a depth first search starting at an arbitrary vertex of the graph. In every step of the DFS, the visited vertex is labeled with the current DFS number. If a back edge is found during the DFS run, the label of the currently visited vertex is updated to the minimum of the labels of both endpoints of the back edge. After the child of a vertex is scanned by the DFS, the algorithm checks whether there were any back edges from the subtree rooted at the child to a predecessor of the vertex in the DFS tree. This is the case if the label of the child is not smaller than the current label of the vertex. If there are no such back edges, then the current vertex is an articulation point.

Eventually, the algorithm has to consider a special case for the root of the DFS tree (i.e. the start vertex). Obviously, there is no predecessor to the root vertex. However, the root can be an articulation point, too. Since there are no cross edges in a DFS tree, this is the case if and only if the root has more than one child. Thus, the root separates the subtrees rooted at the children from each other.

\begin{algorithm}
	\caption{GetArticulationPoints}\label{alg:artic}
	\SetKwFunction{U}{ArticulationPoints}
	\DontPrintSemicolon
	
	\KwIn{A graph $G=(V,E)$}
	
	$AP \leftarrow \emptyset$\; 
	$v\leftarrow u\in V$\tcp*{pick arbitrary start vertex}
	$currentDFSnum \leftarrow 1$ ; $rootDeg \leftarrow 0$\;
	\U{G, v, w}	
	\Begin{
		\If{$v = w$}{$rootDeg \leftarrow rootDeg + 1$\tcp*{$v$ is root}}
		
		$\text{label}(v) \leftarrow currentDFSnum$\;
		$currentDFSnum \leftarrow currentDFSnum + 1$\;
		\ForEach{$u\in N(v)\setminus\{w\}$}
		{
		\eIf{$\text{label}(u) = \bot$} 
		{ \U{G, u}\tcp*{$\{v,u\}$ tree edge}
		$\text{label}(v) = \min\{\text{label}(v),\text{label}(u)\}$\;	
		\If{$\text{label}(u) > \text{label}(v)$}{$AP \leftarrow AP \cup\{v\}$\tcp*{no back edge: $v$ is articulation point}} } 
		{$\text{label}(v) = \min\{\text{label}(v),\text{label}(u)\}$ \tcp*{$\{v,u\}$ back edge}}
		}
		
	}
	\If{$rootDeg < 2$}{$AP \leftarrow AP \cup \{v\}$\tcp*{root is no articulation point}}
	\Return{$AP$}\;
	\KwOut{the set $AP$ of articulation points}
	
\end{algorithm}

\paragraph{}
In our first branching strategy (Algorithm \ref{alg:artic_strat}) we manage a set of articulation points of the graph. During a branching step we first remove vertices from the set that are no longer contained in the graph (e.g. vertices removed by kernelization). Subsequently, we check if the set still contains any articulation points. If so, we remove a vertex from the set and return it for branching. Otherwise, we use Algorithm \ref{alg:artic} to find the articulation points of the graph and insert them into the set. If there are no articulation points we use the default branching strategy as fallback. 

\begin{algorithm}
	\caption{ArticulationPointsBranching}\label{alg:artic_strat}
	\SetKwFunction{U}{ArticulationPointsBranching}
	\SetKwFunction{A}{GetArticulationPoints}
	\SetKwFunction{D}{MaxDegBranching}
	\DontPrintSemicolon
	
	\KwIn{A graph $G=(V,E)$}
	
	\U{G, v}
	\Begin{
		\ForEach{$u \in AP$}{\If{$u\notin V$}{$AP \leftarrow AP\setminus \{u\}$\tcp*{remove vertices no longer contained in $G$}}}
		\If{$AP = \emptyset$}{$AP \leftarrow $ \A{G} \tcp*{search new articulation points in $G$}}
		$v \leftarrow none$\;
		\eIf{$AP = \emptyset$}{$v \leftarrow$ \D{G}\tcp*{use default branching}}{$v\leftarrow u\in AP$ ; $AP\leftarrow AP\setminus\{u\}$\tcp*{use any articulation point for branching}}
		\Return $v$
	}
	
	\KwOut{a vertex $v$ for branching}
	
\end{algorithm}



\paragraph{}
Although this branching strategy has only little overhead, a major drawback  is that articulation points are rarely found even in sparse graphs. Thus, in practice, the fallback strategy is used most of the time. So, our next idea is to also consider more general vertex separators, i.e. minimal separators that contain more than one vertex. 
\paragraph{}
Since minimum edge cuts are generally easier to find than minimum vertex separators and can also be used to obtain small vertex separators, we opted to use edge cuts instead. (In fact, finding a minimum vertex separator can be reduced to finding a minimum edge cut in a transformed graph). A disadvantage of this approach is that vertex separators induced by minimum edge cuts are not necessarily minimal. Nevertheless, for our purpose the trade off between separator size and computation time may be worth it. Given an edge cut, a vertex separator can be obtained easily by just taking one of the incident vertices to each edge in the cut set. However, in our implementations we use a slightly more sophisticated method which yields potentially smaller vertex separators. Given an edge cut $(S,T)$ with cut set $C$, we construct an auxiliary graph $G'=(V',C)$  with $V' = \{x,y\in V\;|\;\{x,y\}\in C\}$. By construction, $G'$ is bipartite with parts $A = V'\cap S$ and $B = V'\cap T$. Thus, we can run the Hopcroft-Karp algorithm \cite{bibid} on $G'$ to obtain a minimum vertex cover $S'$ of $G'$. Since each edge of the cut set is incident to at least one vertex in the vertex cover, $S'$ is indeed a vertex separator. Clearly $S'$ has at most as many vertices as there are edges in $C$.
\paragraph{}
Besides that, the most crucial part of our next branching strategy is the calculation of the actual cut.~Altogether, we test four methods for finding small edge cuts. For our first attempt we use a heuristic algorithm for global minimum cuts by Henzinger et al. \cite{bibid}. Unfortunately, during implementation we noticed that searching a global minimum cut in our benchmark instances almost always results in a trivial cut with a part that only contains a vertex of minimum degree.

Hence, our next approach is  to use $s$-$t$-cuts instead. This naturally raises the question which vertices should be used for $s$ and $t$. At first we try choosing $s$ and $t$ at random. However this results in rather large and unbalanced cuts most of the times. Our next attempt is to use a pair of vertices that are as far apart as possible. The idea behind this is that doing so might produce more balanced cuts. In our implementation we realized this by running a breadth first search (BFS) twice. The first BFS run is started at an arbitrary vertex, and the last vertex visited by the BFS is used for $s$. After that we start another BFS at $s$ and this time the vertex visited last becomes $t$. But choosing $s$ and $t$ this way also results in very unbalanced cuts similar to the global minimum cuts. The reason for this is that the selected vertices almost always have low degree. Finally, we use the two vertices of highest degree as $s$ and $t$. Of all three variants, this delivers the best results on our benchmark instances.

\paragraph{}
To calculate the actual minimum $s$-$t$-cut we use a preflow push maximum flow algorithm. Finally, it has to be considered how to branch on a vertex separator that contains more than one vertex. In our implementation we decide to just branch on each vertex one after another.
\paragraph{}
Bringing all together, our second branching strategy (Algorithm \ref{alg:cut_strat}) manages a set of branching vertices contained in the vertex separator that is currently used for branching. If the set is not empty, the branching strategy just removes and returns a vertex from that set. Otherwise, it searches a new vertex separator. For this, we first retrieve the two vertices $s$ and $t$ in $G$ with highest degree. After that, we use the preflow push algorithm to obtain a minimum $s$-$t$-cut. Using this cut, the branching strategy calculates a vertex separator by constructing the bipartite auxiliary graph induced by the cut set and then applying the Hopcroft-Karp algorithm to it. The new vertex separator (i.e. the minimum vertex cover returned by the Hopcroft-Karp algorithm) is then inserted in the set of branching vertices and one of those vertices is returned.

It is an important optimization that the branching strategy only considers vertex separators which are not larger than a certain size and which also meet certain balancing constraints. Furthermore, we noticed that if no suitable vertex separator is found, this is also the case in the next couple of branches. For this reason, the branching strategy only searches for a vertex separator every x branching steps to reduce the overhead and uses the default fallback during this phase. The exact numbers used in our implementation are tuning parameters. The details of choosing those tuning parameters are discussed in Section \ref{sec5} and are omitted in the pseudo code. 

\begin{algorithm}
	\caption{CutBranching}\label{alg:cut_strat}
	\SetKwFunction{U}{CutBranching}
	\SetKwFunction{A}{GetArticulationPoints}
	\DontPrintSemicolon
	
	\KwIn{A graph $G=(V,E)$}
	
	\U{G, v}	
	\Begin{
		TODO
	}
	
	\KwOut{a vertex $v$ for branching}
	
\end{algorithm}
\paragraph{}
Both of our branching strategies so far perform the calculation of the branching vertex dynamically in every branching step. This has the advantage that branching vertices are always chosen based on the current graph but comes with the disadvantage of computation overhead in every branching step. Thus, our next approach is to use a static ordering in which the vertices are considered for branching and that is calculated only once before the first branching step. For this purpose we decided to use nested dissection ordering.

\paragraph{}
Nested dissection ordering is mainly used as a heuristic to minimize the number of fill ins in factorization of sparse symmetric matrices or for computing good contraction hierarchies in route planning.  A nested dissection ordering of the vertices of a graph $G$ is obtained by calculating a balanced bipartition of the graph into parts $A$ and $B$ using a vertex separator $S$. Subsequently, orderings of $G_A$ and $G_B$ are calculated recursively. Finally, the nested dissection ordering of $G$ is composed by concatenating the orderings of $A$ and $B$ followed by the separator $S$. For vertices of the separator $S$, the relative order among each other is arbitrary. By choosing the branching vertices in reverse nested dissection ordering, the algorithm branches on the separators used for the bipartition at each level of the nested dissection. Consequently, the graph is getting decomposed piece by piece.

\paragraph{}
In the implementation of our following branching strategy (Algorithm \ref{label}) a nested dissection ordering is computed immediately prior to the first branching step, i.e., after the initial kernelization step. To calculate the nested dissection ordering, we use an algorithm provided in the METIS library~\cite{bibid}. We do not apply a custom configuration to the algorithm but rather use the default settings of the METIS library. Afterwards, the vertices are inserted into a queue in reverse nested dissection ordering. In each branching step, the branching strategy removes vertices from the queue until it finds a vertex that is still contained in the current graph and, finally, returns the respective vertex for branching.
\paragraph{} 
During initial tests of our implementation we noticed that branching on separators which come later in the reversed nested dissection ordering (i. e. separators obtained in a recursive call of higher depth), frequently does not result in the decomposition of the graph. This is due to the fact that, in addition to branching, the graph is also reduced by kernelization in each step of the algorithm. Thus, separators in the original graph are not necessarily separators in the current graph at the time of branching. Counteracting this, we optimized our branching strategy by performing a restricted number of levels of recursions instead of calculating a full nested dissection ordering. Then, the branching order is composed solely by the separators obtained in those levels of recursion. In a branching step, if there are no more vertices left in the ordering, the default branching strategy is used as a fallback. The exact number of recursions used is a tuning parameter. Details are discussed in the following section (Section \ref{sec5}).
\paragraph{}
To implement this optimization, we used another method provided by the METIS library, which just performs a specified number of resursive calls of the nested dissection algorithm. Nevertheless, a problem is that the algorithm still returns an ordering of all vertices of the graph. We are, however, only interested in the vertices contained in the separators of each recursive call. Luckily, the METIS library provides an array that stores the sizes of the parts as well as the separators for each level of recursion. The following recursive algorithm (Algorithm~\ref{label}) capitalizes on this feature by extracting the individual separators from the vertex ordering and constructing the branching ordering in the following manner.
 
\paragraph{}
The algorithm receives the vertex ordering and the sizes array as input. Initially, the branching order is empty. In each recursive call the algorithm starts by retrieving the size $k$ of the top level separator from the sizes array. Thereafter, the top level separator, which just contains the last $k$ vertices of the vertex ordering, is inserted into the branching ordering. Then, the algorithm uses the part and separator sizes to split the rest of the vertex ordering and the sizes array in accordance to the recursion. This is going to result in orderings for each part of the bipartition and the two corresponding sizes arrays. Subsequently, the separators residing in those parts are extracted recursively and are added to the branching order.
\paragraph{}
Just as in the previous branching strategy using edge cuts, we optimize our implementation by considering a vertex ordering only in case if the sizes of the vertex separators computed at each level of recursion are not exceeding a certain threshold. The exact value of the threshold is a tuning parameter. In contrast to the previous branching strategy, we did not tweak the balancing constraints used by the nested dissection algorithm. 

\paragraph{}
Another optimization we tested was to compute a new nested dissection ordering (with restricted levels of recursion), upon removal of all vertices of the previous ordering from the graph. This way, we combine the advantages of using a static branching ordering and choosing vertices dynamically on the current graph.  However, tests showed that this variant of the branching strategy performs worse than computing the vertex ordering only once. A possible explanation is that branches at a lower depth of recursion have a greater impact on the total number of branches needed than such at a higher depth. Hence, decomposing the graph at an early stage in the algorithm is more powerful than in later stages. An alternate reason might be that branches at a higher depth of recursion are more likely to be pruned before all vertices of a separator have been removed by branching. 

\subsection{Branching Strategies Based on Reduction Rules} \label{red_strats}

The branching strategies described in this section are essentially based on the destruction of structures that can not be reduced by kernelization. The core idea behind this is to identify vertices such that branching on those enables the application of reduction rules afterwards. Our initial approaches using this concept are rather simple and attempt to find single vertices that prevent a certain reduction rule from being applicable. It is generally advantageous that finding such vertices can be accomplished during kernelization without noticeable time overhead.
\paragraph{}
Following this idea, we implemented and tested four branching strategies each targeting a different reduction rule. Finding of potential candidates for branching, i.e. vertices that prevent the corresponding reduction rule from being applicable, works differently depending on the targeted reduction rule. However, the actual branching step is the same for all these variants. In a branching step, a vertex for branching is chosen from a set of vertices managed by the respective branching strategies. Vertices considered for branching are inserted into this set during kernelization. Notice, that at the time of branching, vertices in the set could have already been removed from the graph by another reduction rule. Thus, reduced vertices have to be filtered out first. Subsequently, if the set of branching vertices is not empty, all four branching strategies choose a vertex of highest degree from the set and return it for branching. Otherwise, if there are no vertices in the set, the default branching strategy is used as a fallback.

\paragraph{}
As in the case of the branching strategies described in Subsection \ref{decomp}, our implementations are optimized to the effect that even if the set of potential branching vertices is not empty, only vertices of a certain degree are considered for branching. Details of how the tuning parameter is chosen are explained in Section \ref{sec5}.

\paragraph{}
In the following, we describe for each of the four branching strategies tested how vertices considered for branching are found during kernelization.

\paragraph{}
The first branching strategy in this subsection is based on the Twin reduction rule (coverd in Subsection~\ref{red}). To characterize the structure in which a single vertex prevents the twin reduction from being applicable, consider the following definition.

\begin{definition} (Almost Twins)
	In a graph $G=(V,E)$ two non adjacent vertices $u$ and $v$ are called almost twins if $d(u) = 4$, $d(v) = 3$ and $N(v)\subseteq N(u)$ (i.e. $N(u) = N(v) \cup \{w\}$). 
\end{definition}

Clearly, after removing $w$, it holds that $d(u)=d(v)=3$ and $N(u)=N(v)$. Thus, by removing $w$, the vertices $u$ and $v$ become twins. Consequently, the twin reduction rule is applicable afterwards. Thus, when using this branching strategy, the branch and reduce algorithm searches for pairs of vertices that are almost twins during each kernelization step. Finding those vertices can be done while already searching for twins. The twin reduction rule checks for each vertex $v$ of degree three if there is a vertex $u \in N^2(v)$ such that $d(u) = 3$ and $N(u)=N(v)$. To find almost twins, we modify this routine in order to simultaneously check if there is a vertex $u \in N^2(v)$ with $d(u) = 4$ and $N(v)\subseteq  N(u)$. If such a pair of vertices $u$ and $v$ is found, the single vertex $w\in (N(u)\cup N(v))\setminus(N(u) \cap N(v))$ is inserted into the set of vertices considered for branching.
\paragraph{}

For the second branching strategy we attempt to find vertices preventing the application of the alternative reduction. As mentioned in Subsection~\ref{red}, the branch and reduce algorithm only utilizes the special cases Funnel and Desk reduction, hence providing a further restriction in terms of eligible reduction rules. However, desk reduction is based on a very concrete graph structure and therefore is rarely applied in our benchmark instances (in contrast, funnel reduction is applied relatively often). Also, it is difficult to determine if a vertex prevents the desk reduction from being applicable, since a multitude of cases would have to be considered.
For those reasons, we do not target the desk reduction but exploit the funnel reduction rule, instead.
\paragraph{}
Similar to the first branching strategy, we define the structure within which the funnel reduction can be applied upon removing a single vertex.
\begin{definition}(Almost Funnel)
	In a Graph $G=(V,E)$ two adjacent vertices $u$ and $v$ are called almost funnels if $u$ and $v$ are not funnels and there is a vertex $w$ such that $N(v)\setminus\{u,w\}$ induces a clique.
\end{definition}

By removing the single vertex $w$, $u$ and $v$ become funnels and, thus, $u$ can be reduced afterwards. Pairs of vertices that are almost funnels can be found easily during the funnel reduction. To check whether two vertices $u$ and $v$ are funnels, the funnel reduction rule iterates over the vertices in $N(v)\setminus \{u\}$ and checks if they are adjacent to the previous vertices in the iteration. Once it finds a vertex that is not adjacent to all prior vertices, the reduction rule concludes that $u$ and $v$ are not funnels and stops checking the remaining neighbors of $v$. To find almost funnels, we modify this procedure as follows. We still iterate over $N(v)\setminus\{u\}$ and check for adjacency. Once a vertex $w$ is found that is not adjacent to all prior vertices, two cases are considered. Firstly, if $w$ is not adjacent to at least two preceding vertices, then, $u$ and $v$ representing almost funnels can be verified by checking if $N(v)\setminus\{u,w\}$ induces a clique. In the second case there is only one vertex $w'$ prior to $w$ such that $w'$ and $w$ are not neighbors. Then, $u$ and $v$ are almost funnels if $N(v)\setminus\{u,w\}$ or $N(v)\setminus\{u,w'\}$ induces a clique. In both cases, if $u$ and $v$ are almost funnels, the neighbor of $v$ which is not contained in the induced clique (i.e. $w$~or~$w'$ respectively) is inserted into the set of vertices considered for branching.

\paragraph{}
The third branching strategy is making use of the dominance reduction rule. Again, we define the structure which contains a vertex that enables dominance reduction upon removal.

\begin{definition} (Almost Dominance)
	In a Graph $G=(V,E)$ a vertex $u$ is called almost dominated by a neighbor $v$ if $|N(u)\setminus N(v)| = 1$ and $N[v]\not\subseteq N(u)$.
\end{definition}

The condition $N[v]\not\subseteq N(u)$ ensures that $v$ is not already dominated by $u$. By removing the single vertex $w\in N(u)\setminus N(v)$, the vertex $u$ becomes dominated by $v$, since $(N[u]\setminus\{w\})\subseteq N(v)$. Similar to the previous branching strategy, we search for pairs of vertices $u$ and $v$ such that $u$ is almost dominated by $v$. The dominance reduction verifies whether a vertex $u$ is dominated by $v$ through checking if all neighbors of $u$ are also neighbors of $v$. Once it finds a vertex that is adjacent to $u$ but not a neighbor of $v$, it is concluded that $u$ is not dominated by $v$ and the remaining vertices are not going to be checked anymore. We modify the dominance reduction to test if the neighbors of $u$ are adjacent to $v$ until either all neighbors have been checked or, alternatively, a second vertex adjacent to $u$ is found that is not also a neighbor of $v$. When all vertices have been checked and there is no vertex $w \in N(u)\setminus N(v)$, then $u$ is dominated by $v$. Otherwise, $w$ is inserted into the set of vertices considered for branching.

\paragraph{}
A fundamental difference to the two preceding branching strategies is that the dominance reduction rule is not actually used by the branch and reduce algorithm. Nevertheless, dominance reduction is fully contained in the unconfined reduction rule used by the algorithm so that dominating vertices still get reduced. It is a major drawback, however, that vertices considered for branching such that the dominance rule becomes applicable, can not be found during kernelization. Instead, one has to search for such vertices separately after kernelization resulting in a non-negligible time overhead. 

\paragraph{}
Therefore, in the fourth branching strategy we generalize the previous strategy in order to exploit the unconfined reduction rule. We define an almost unconfined vertex analogously to the previous definitions.

\begin{definition} (Almost Unconfined)
	In a graph $G=(V,E)$ a vertex $v$ is called almost unconfined if $v$ is not unconfined but there is a vertex $u$ such that $v$ is unconfined in $G-u$.
\end{definition}

By definition $v$ becomes unconfined after removing $u$ and, thus, by means of the unconfined reduction rule can also be removed. However, it is not clear how to determine whether a vertex $u$ is almost unconfined and, moreover, how to find that specific vertex $u$ preventing $v$ from being unconfined. There are basically two cases to be considered. Firstly, at some point during the execution of the unconfined algorithm (Algorithm \ref{alg:unconf}) there is an extending child, i. e., a child $w$ with $\{u\} = N(w) \setminus N[S]$, and, coincidentally, inclusion of $u$ into the set $S$ leads to $v$ not being unconfined. By removing $u$, the vertex $w$ becomes a child devoid of neighbors not being already contained in $N[S]$. Thus, $v$ becomes unconfined. In the second case, at the end of the unconfined algorithm there is a child $w$ with $\{u,x\} = N(w) \setminus N[S]$. Upon removing $u$, the vertex $w$ becomes an extending child. Therefore, the unconfined algorithm has to include $x$ into the set $S$ and eventually concludes that $v$ is unconfined. 

It is easy to check whether the first case occurs. In each step of the unconfined algorithm, if there is only one extending child $w$ with $\{u\} = N(w) \setminus N[S]$, we insert $u$ into a buffer. When the algorithm terminates, the following condition applies. If it returns false and the buffer is not empty, then $v$ is almost unconfined and removal of any vertex from the buffer makes $v$ unconfined. Hence, all vertices from the buffer can be considered for branching.

Notably, there is an obstacle in detecting the second case. If the unconfined algorithm concludes a vertex $v$ is not unconfined, we can in fact check if at the end of the unconfined algorithm there is a child $w$ with $\{u,x\} = N(w) \setminus N[S]$; but we do not know if the removal of either $u$ or $x$ is going to result in $v$ becoming unconfined. Hence, in this case we can not easily check whether $v$ is almost unconfined or not.

\paragraph{}
Thus, during kernelization almost unconfined vertices can only be identified if case one applies. Unfortunately, it is not clear whether this is true if a vertex $u$ is almost dominated by another vertex $v$. It holds that $v$ is either unconfined (then $v$ is reduced anyway) or, alternatively, $v$ is almost unconfined since by removing the single vertex $w \in N(u)\setminus N(v)$, $u$ becomes dominated by $v$; hence, $v$ becomes unconfined. Also, during the unconfined procedure, $u$ is an extending child as long as the set $S$ does not contain $w\in N(u)\setminus N(v)$ following from $N(u)\setminus\{w\}\subseteq N(v)$. Hence, if $u$ is the only extending child at any point during the unconfined algorithm, then case one applies. However, there can also be another extending child being considered prior to $u$ by the unconfined algorithm, consequently forcing $w$ into the set $S$. In this instance, $u$ is no longer a child of $S$ and, eventually, case two might prevail. 
Moreover, if a vertex $v$ is almost unconfined, there can also be a vertex such that removing the latter results in $v$ being neither unconfined nor almost unconfined anymore. As an example see Figure \ref{label}. Notably, this vertex can also be removed during a branching step by the mirror branching rule (covered in Subsection \ref{red}).

\paragraph{}
During initial test runs using the first four branching strategies covered in this section, we observed that branching on a vertex may result in a whole series of reductions. There are two main causes for this. First, within one branching step multiple vertices get frequently removed from the graph. Specifically, this is the case when the branching vertex is included into the current solution and therefore its neighbors are excluded or, alternatively, when the branching vertex has mirrors (see Mirror branching rule in subsection \ref{red}). Then, each of the removed vertices might enable a reduction during the following kernelization. The second cause is that removal of a vertex during branching might lead to a reduction which, in turn, enables further reductions. 
\paragraph{}
We make use of this observation in our fifth branching strategy. Here, the main idea is to choose a vertex such that branching on this vertex is going to trigger a largest possible chain of reductions. Consider the following example shown in Figure \ref{label}. 

\paragraph{}
To find large reduction chains, we define the following directed graph $R=(V, E^\prime)$, where $V$ is just the vertex set of the original graph; further there is an edge from a vertex $u$ to another vertex $v$ if the removal of $u$ causes the reduction of $v$ based on a single reduction rule (we omit transitive edges). We call this a reduction graph. Given the reduction graph $R$, the number of vertices, which can be reduced after removing a vertex $v$, at best corresponds to the number of vertices reachable from $v$ in $R$. We can compute this number with a simple BFS run starting at $v$.

\paragraph{}
Unfortunately, in practice the exact reduction graph is hard to construct as there are a few pitfalls. We can in fact use the results of our previous branching strategies to identify vertices that enable the reduction of other vertices upon their removal so that parts of the reduction graph can be constructed. However, in doing so, we do not consider all reduction rules used by the branch and reduce algorithm. Also, we disregard the order in which the reduction rules are applied. Since the reduction rules are not executed iteratively but rather in a fixed order, it is therefore not guaranteed that the whole chain of reductions is performed within one kernelization step. Folding reductions (e.g., degree two folding or twin reduction, respectively) add new vertices while alternative reductions introduce new edges between existing vertices. Consequently, those reductions might prevent other reductions. Moreover, Somebody et al. [Quelle fehlt noch] show that the order in which reduction rules are applied may affect the kernel size. Thus, the number of vertices reachable in the reduction graph starting at a specific vertex does not necessarily correspond to the exact number of reductions caused by the removal of that vertex.

\paragraph{}
In our implementation, we only consider almost dominated and almost unconfined vertices for constructing an approximation to the reduction graph, as the unconfined reduction does not introduce new vertices or edges. We use the modified variants of the dominance and unconfined reduction to find vertices that enable reductions upon removal. In a branching step, initially, the reduction graph is constructed. Subsequently, we perform BFS runs starting at each vertex of the graph to compute the number of vertices reachable. These numbers are utilized to find the vertex starting from which the highest number of vertices are reachable. The same vertex is finally used for branching if it has a certain degree.

\newpage
\section{Experimental Results and Conclusions} \label{sec5}
\newpage
\section{Future Work} \label{sec6}
In this section we discuss possible options to further develop and optimize our branching strategies described in this thesis. Moreover, we outline new ideas for alternate approaches for branching strategies.

\paragraph{}
As already mentioned in Section \ref{sec5}, the aim of this thesis was not to optimize certain strategies down to the last detail. Instead, we examined a range of different strategies following multiple approaches in an attempt to comparatively evaluate the potential of those solutions. Thus, for most of our implementations, there is a lot of leeway to optimize runtime. For instance, we did not implement our own variant of a preflow push algorithm, but instead used an implementation from the KaHIP library \cite{bibid}. Consequently, the current graph has to be converted to the format used by the KaHIP library at each branching step, which creates additional overhead.

\paragraph{}
Our test results show that branching on articulation points is ineffective most of the time. However, there are two instances for which this strategy produces significantly better results than default branching. Unfortunately, we did not find a satisfying explanation for these outliers. Analyzing different graph parameters on the initial kernel of those instances showed that both outliers seem to have a higher average betweenness centrality than other instances of their respective graph class. However, when looking at all instances together, we could not find any correlation between the effectiveness of this branching strategy and betweenness centrality. Nevertheless, we believe that exploring relations between the effectiveness of branching strategies and graph characteristics can help optimizing existing strategies or even finding new ones. Stallmann, Ho and Goodrich previously proposed a similar approach involving reduction rules \cite{bibid}. They showed that there is a correlation between certain graph parameters and the effectiveness of the reduction rules. Furthermore, they presented an improved variant of the branch and reduce algorithm by Akiba and Iwata \cite{AkibaIwata} where they only apply a subset of reduction rules based on simple graph parameters of the instance. This way, they reduced kernelization time and significantly speeded up the algorithm.

\paragraph{}
Nested dissection has proven to work very well, at least on DIMACS instances. Therefore, we see a lot of potential for further optimization involving this strategy. The METIS library provides many settings that can be used to tweak the nested dissection algorithm, and thus the branching strategy. Further, it might be interesting to examine whether there is a tradeoff between the balance of the parts and the size of the separators. Also, a partitioning algorithm that produces higher quality bipartitions at the cost of computation time could improve the strategy. 

\paragraph{}
Our experiments show that the branching strategy, which targets the dominance reduction, significantly reduces the total number of branches needed on PACE and DIMACS instances. Unfortunately, the overhead in computation time it takes for finding almost dominated vertices is excessively large. Moreover, it is not clear whether those vertices can be found with our proposed modification of the unconfined reduction. Potentially, this problem could be fixed by considering extending children in the order of their appearance.

\paragraph{}
We believe that the concept of branching aiming at the application of reduction rules can also be adopted for the packing branching rule. During kernelization one can find vertices whose removal forces the reduction of other vertices in order to satisfy the constraints resulting from the packing rule. This could also be utilized in the branching strategy that targets chain reductions, since packing reduction does not introduce new vertices or edges. 

\paragraph{}
During the initial testing of our branching strategy using nested dissection ordering, we noticed that computing the nested dissection ordering at later stages is not advantageous. A possible explanation is that branches at lower depth of recursion have a greater influence on the running time of the algorithm than such at a higher depth. For this reason it might be worth investing a lot of execution time into finding branching vertices at the start of recursion compensating for the time spent by using a simple branching strategy at higher depth of recursion.

\newpage
\bibliographystyle{plain}
\bibliography{literatur}


\end{document}
